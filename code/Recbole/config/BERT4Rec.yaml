# Data Settings
field_separator: "\t"
seq_separator: " "
USER_ID_FIELD: user_id
ITEM_ID_FIELD: item_id
TIME_FIELD: timestamp
LABEL_FIELD: label
rm_dup_inter: False
user_inter_num_interval: "[0, inf)"
item_inter_num_interval: "[0, inf)"
load_col:
    inter: [user_id, item_id, timestamp, label]
    user : [user_id]
    item : [item_id, year, genre, writer, director]
ITEM_LIST_LENGTH_FIELD: item_length
LIST_SUFFIX: _list
MAX_ITEM_LIST_LENGTH: 50

# Environment Settings
gpu_id: 0
use_gpu: True
seed: 2020
state: 'INFO'
encoding: 'utf-8'
reproducibility: True
data_path: data
checkpoint_dir: 'model/'
show_progress: True
save_dataset: False
dataset_save_path: None
save_dataloaders: False
dataloaders_save_path: None
log_wandb: True
wandb_project: 'RecBole_BERT4Rec'

# Training Settings_basic
epochs: 200
train_batch_size: 512
learner: 'adam'
learning_rate: 0.0005
neg_sampling: None # non-sampling model
train_neg_sample_args: ~
eval_step: 1
stopping_step: 10
loss_decimal_place: 4
weight_decay: 0.000

#Evaluation Settings
eval_args:
    split: {"LS": 'valid_and_test'}
    group_by: user
    order: TO
    mode: full
repeatable: True
metrics: ['Recall', 'MRR', 'NDCG', 'Hit', 'Precision']
topk: [10]
valid_metric: Recall@10
eval_batch_size: 4096
metric_decimal_place: 4

# Model settings
n_layers: 2                     # (int) The number of transformer layers in transformer encoder.
n_heads: 2                      # (int) The number of attention heads for multi-head attention layer.
hidden_size: 64                 # (int) The number of features in the hidden state.
inner_size: 256                 # (int) The inner hidden size in feed-forward layer.
hidden_dropout_prob: 0.2        # (float) The probability of an element to be zeroed.
attn_dropout_prob: 0.2          # (float) The probability of an attention score to be zeroed.
hidden_act: 'gelu'              # (str) The activation function in feed-forward layer.
layer_norm_eps: 1e-12           # (float) A value added to the denominator for numerical stability.
initializer_range: 0.02         # (float) The standard deviation for normal initialization.
mask_ratio: 0.1                 # (float) The probability for a item replaced by MASK token.
loss_type: 'CE'                 # (str) The type of loss function.
transform: mask_itemseq         # (str) The transform operation for batch data process.
ft_ratio: 0.5                   # (float) The probability of generating fine-tuning samples
model: BERT4Rec
dataset: movie
